{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "O7Bjj4a26pBR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torchmetrics.classification import MulticlassAccuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV2_O66W6pBU"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "r80i4aP-6pBV",
        "outputId": "fda8260b-b964-4c8f-e33a-426a16638699"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>websites &amp; communications terms of use</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>please read the terms of this entire document ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>by accessing or signing up to receive communic...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>our websites include multiple domains such as ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you may also recognize our websites by nicknam...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20412</th>\n",
              "      <td>you may terminate your account at our service ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20413</th>\n",
              "      <td>the following provisions shall survive termina...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20414</th>\n",
              "      <td>the company reserves the right , at its sole d...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20415</th>\n",
              "      <td>your continued use of the service , following ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20416</th>\n",
              "      <td>the last revision will be reflected in the `` ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20417 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "0                 websites & communications terms of use      0\n",
              "1      please read the terms of this entire document ...      0\n",
              "2      by accessing or signing up to receive communic...      4\n",
              "3      our websites include multiple domains such as ...      0\n",
              "4      you may also recognize our websites by nicknam...      0\n",
              "...                                                  ...    ...\n",
              "20412  you may terminate your account at our service ...      0\n",
              "20413  the following provisions shall survive termina...      0\n",
              "20414  the company reserves the right , at its sole d...      3\n",
              "20415  your continued use of the service , following ...      4\n",
              "20416  the last revision will be reflected in the `` ...      0\n",
              "\n",
              "[20417 rows x 2 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('Dataset/ToS-100-cleaned.csv',header=0,encoding='utf-8')\n",
        "data = df[['text','label']]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCf939K6pBV"
      },
      "source": [
        "## Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7zE__vg46pBV"
      },
      "outputs": [],
      "source": [
        "text = data['text'].values.tolist()\n",
        "labels = data['label'].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InUKUGRA6pBW"
      },
      "source": [
        "#### HyperParameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9g7K3WxS6pBW"
      },
      "outputs": [],
      "source": [
        "embd_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtbFUYQ36pBW"
      },
      "source": [
        "## Tokenization and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSPPCL2Z6pBW",
        "outputId": "0f8b6ab5-ac09-401e-b298-f2bd0f106aa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/joeliang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(tos.lower()) for tos in text]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "embedding = Word2Vec(sentences=tokenized_sentences, vector_size=embd_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "if not os.path.exists(\"Model/baseline\"):\n",
        "    os.makedirs(\"Model/baseline\")\n",
        "embedding.save(\"Model/baseline/word2vec_model_tos\")\n",
        "\n",
        "# Access word embeddings\n",
        "word_embeddings = embedding.wv\n",
        "tos_embeddings = [[word_embeddings[word] for word in tos] for tos in tokenized_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "l_YIdy0D6pBX",
        "outputId": "c7e17fa5-91d9-49e3-d14e-0d1bc86deeb5",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of sentences longer than 128: 212\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# collect the length of sentences\n",
        "# used to decide on a practical sequence_length for training\n",
        "length_data = [len(tos) for tos in tokenized_sentences]\n",
        "\n",
        "plt.hist(length_data, bins='auto', alpha=0.7, color='blue', edgecolor='black')\n",
        "plt.show()\n",
        "\n",
        "# no of sentences longer than threshold\n",
        "threshold = 128\n",
        "print(f\"number of sentences longer than {threshold}: {len([l for l in length_data if l > threshold])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al15ZlVj6pBX"
      },
      "source": [
        "## Clean the dataset\n",
        "\n",
        "* Crop sentences longer than seq_length.\n",
        "* Add padding to the beginning of other sentences.\n",
        "* Fix the class imbalance between fair and unfair sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPGcgRtU6pBX",
        "outputId": "aedf7c14-2c91-499b-9832-20073953e7c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After SMOTE sampling\n",
            "number of fair sentences: 18235\n",
            "number of unfair sentences: 18235\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "max_length = 128\n",
        "def crop_and_pad(sentence, seq_length=max_length):\n",
        "    length = len(sentence)\n",
        "\n",
        "    if length > seq_length:\n",
        "        new_sentence = sentence[:seq_length]\n",
        "    else:\n",
        "        new_sentence = [np.zeros(embd_size)]*(seq_length - length) + sentence\n",
        "\n",
        "    return new_sentence\n",
        "\n",
        "# crop and pad\n",
        "processed_embeddings = np.stack([crop_and_pad(tos) for tos in tos_embeddings], axis = 0)\n",
        "\n",
        "# class balancing\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "indices = [[i] for i in range(len(labels))] # smote requires 2-D array!\n",
        "\n",
        "indices, labels = smote.fit_resample(indices, labels) # smote cannot work on text data, so passing indices\n",
        "\n",
        "processed_embeddings = [torch.tensor(processed_embeddings[i[0]], dtype=torch.float32) for i in indices]\n",
        "labels = torch.tensor(labels, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After SMOTE sampling\n",
            "class 0: 18235\n",
            "class 1: 18235\n",
            "class 2: 18235\n",
            "class 3: 18235\n",
            "class 4: 18235\n"
          ]
        }
      ],
      "source": [
        "# make sure class balance is achieved\n",
        "print(\"After SMOTE sampling\")\n",
        "print(f\"class 0: {len([label for label in labels if label == 0])}\")\n",
        "print(f\"class 1: {len([label for label in labels if label == 1])}\")\n",
        "print(f\"class 2: {len([label for label in labels if label == 2])}\")\n",
        "print(f\"class 3: {len([label for label in labels if label == 3])}\")\n",
        "print(f\"class 4: {len([label for label in labels if label == 4])}\")\n",
        "\n",
        "\n",
        "dataset = list(zip(processed_embeddings, labels))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO3gIwBJ6pBY"
      },
      "source": [
        "## Classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_kySsEHl6pBY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(dataset_path='Dataset/ToS-100-cleaned-dataset-huggingface')\n",
        "train = pd.DataFrame(dataset['train'])\n",
        "test = pd.DataFrame(dataset['test'])\n",
        "train, validate = train_test_split(train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_target(y_batch,device):\n",
        "    target = np.zeros((len(y_batch), 5))\n",
        "    index = 0\n",
        "    for y in y_batch:\n",
        "        target[index][y] = 1\n",
        "        index += 1\n",
        "        target = torch.tensor(target).to(torch.float).to(device)\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, df):\n",
        "        # X: torch.tensor (maxlen, batch_size), padded indices\n",
        "        # Y: torch.tensor of len N\n",
        "        X, Y = [], []\n",
        "        V = len(vocab.vectors)\n",
        "        for i, row in df.iterrows():\n",
        "            L = row[\"text\"].split()\n",
        "            X.append(torch.tensor([vocab.stoi.get(w, V-1) for w in L]))  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
        "            Y.append(row.label)\n",
        "        self.X = X\n",
        "        self.Y = torch.tensor(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "    \n",
        "def my_collate_function(batch, device):\n",
        "    # Handle the padding here\n",
        "    # batch is approximately: [dataset[i] for i in range(0, batch_size)]\n",
        "    # Since the dataset[i]'s contents is defined in the __getitem__() above, this collate function\n",
        "    # should be set correspondingly.\n",
        "    # Also: collate_function just takes one argument. To pass in additional arguments (e.g., device),\n",
        "    # we need to wrap up an anonymous function (using lambda below)\n",
        "    batch_x, batch_y = [], []\n",
        "    max_len = 0\n",
        "    for x,y in batch:\n",
        "        batch_y.append(y)\n",
        "        max_len = max(max_len, len(x))\n",
        "    for x,y in batch:\n",
        "        x_p = torch.concat(\n",
        "            [x, torch.zeros(max_len - len(x))]\n",
        "        )\n",
        "        batch_x.append(x_p)\n",
        "    return torch.stack(batch_x).t().int().to(device), torch.tensor(batch_y).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    #   fix seed\n",
        "    torch.manual_seed(2)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print (\"Using device:\", device)\n",
        "\n",
        "    glove = torchtext.vocab.GloVe(name=\"6B\",dim=100)\n",
        "\n",
        "    train_dataset = TextDataset(glove, train)\n",
        "    val_dataset = TextDataset(glove, validate)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "    validation_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "    # Instantiate your model(s) and train them and so on\n",
        "    # We suggest parameterizing the model - k1, n1, k2, n2, and other hyperparameters\n",
        "    # so that it is easier to experiment with\n",
        "\n",
        "    class CNN(nn.Module):\n",
        "      def __init__(self, embedding_dim, k1, k2, n1, n2):\n",
        "          super(CNN, self).__init__()\n",
        "\n",
        "          self.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
        "          self.layer1 = nn.Conv2d(1,n1,kernel_size=(k1,embedding_dim),bias = False)\n",
        "          self.activition = torch.nn.ReLU()\n",
        "\n",
        "          self.layer2 = nn.Conv2d(1,n2,kernel_size=(k2,embedding_dim),bias = False)\n",
        "          self.linear = torch.nn.Linear(in_features=n1+n2,out_features= 5)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self.embedding(x).permute(1, 0, 2)\n",
        "        x = torch.unsqueeze(x, 1)\n",
        "\n",
        "        x1 = self.layer1(x)\n",
        "        x1  = self.activition(x1).squeeze(3)\n",
        "        x1 = nn.functional.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
        "\n",
        "        x2 = self.layer2(x)\n",
        "        x2 = self.activition(x2).squeeze(3)\n",
        "        x2 = nn.functional.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
        "        x  = torch.cat((x1,x2),1)\n",
        "\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def train_cnn(lr_rate,epochs, k1, k2, n1, n2):\n",
        "      network = CNN(100, k1, k2, n1, n2).to(device)\n",
        "      criterion = nn.CrossEntropyLoss().to(device)\n",
        "      optimizer = torch.optim.Adam(network.parameters(),lr = lr_rate)\n",
        "      metric = MulticlassAccuracy(num_classes=5).to(device)\n",
        "\n",
        "      trainepoch_loss = []\n",
        "      valepoch_loss = []\n",
        "      trainepoch_accurcy = []\n",
        "      valepoch_accurcy = []\n",
        "\n",
        "        # validation dataset\n",
        "      def evaluation(network):\n",
        "          valid_loss = []\n",
        "          valid_accuracy = []\n",
        "          for i, (X_batch, y_batch) in enumerate(validation_dataloader):\n",
        "            X_batch.to(device)\n",
        "            target = get_target(y_batch,device)\n",
        "            output = network(X_batch)\n",
        "            loss_val = criterion(output,target.squeeze(1))\n",
        "            # loss and accuracy\n",
        "            probs = torch.sigmoid(output)\n",
        "            valid_accuracy.append(metric(probs, target).item())\n",
        "            valid_loss.append(loss_val.item())\n",
        "          return valid_accuracy,valid_loss\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          train_loss = []\n",
        "          train_accuracy = []\n",
        "\n",
        "          for i, (X_batch, y_batch) in enumerate(train_dataloader):\n",
        "            target = get_target(y_batch,device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = network(X_batch)\n",
        "            loss = criterion(output,target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss and accuracy\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            probs = torch.sigmoid(output)\n",
        "            train_accuracy.append(metric(probs, target).item())\n",
        "\n",
        "          # running evaluation function once per two epoch\n",
        "          if (epoch % 2 == 0):\n",
        "            valid_accuracy,valid_loss = evaluation(network)\n",
        "\n",
        "\n",
        "          print(f'\\t\\t ------- Epoch {epoch+1} ---------')\n",
        "          print(f'\\t\\t Training Loss: {np.mean(train_loss)} \\t\\t Training Accurcy: {np.mean(train_accuracy)}')\n",
        "          print(f'\\t\\t Validation Loss: {np.mean(valid_loss)} \\t\\t Validation Accurcy: {np.mean(valid_accuracy)}')\n",
        "\n",
        "          trainepoch_loss.append(np.mean(train_loss))\n",
        "          valepoch_loss.append(np.mean(valid_loss))\n",
        "\n",
        "          trainepoch_accurcy.append(np.mean(train_accuracy))\n",
        "          valepoch_accurcy.append(np.mean(valid_accuracy))\n",
        "\n",
        "      return trainepoch_loss,trainepoch_accurcy,valepoch_loss,valepoch_accurcy,network\n",
        "\n",
        "    trainepoch_loss,trainepoch_accurcy,valepoch_loss,valepoch_accurcy,network = train_cnn(0.003,40,args.k1,args.k2,args.n1,args.n2)\n",
        "    return trainepoch_loss,trainepoch_accurcy,valepoch_loss,valepoch_accurcy,network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===10===2===4===60\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v4/nb6fv1kd5zjc4dx7n_vnd8mr0000gn/T/ipykernel_70595/2189369551.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(target).to(torch.float).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\t ------- Epoch 1 ---------\n",
            "\t\t Training Loss: 0.4128002048292115 \t\t Training Accurcy: 0.5\n",
            "\t\t Validation Loss: 0.21851379037451968 \t\t Validation Accurcy: 0.5\n",
            "\t\t ------- Epoch 2 ---------\n",
            "\t\t Training Loss: 0.1578863416119074 \t\t Training Accurcy: 0.5\n",
            "\t\t Validation Loss: 0.21851379037451968 \t\t Validation Accurcy: 0.5\n",
            "\t\t ------- Epoch 3 ---------\n",
            "\t\t Training Loss: 0.08868737553442639 \t\t Training Accurcy: 0.5\n",
            "\t\t Validation Loss: 0.09174735820373801 \t\t Validation Accurcy: 0.5\n",
            "\t\t ------- Epoch 4 ---------\n",
            "\t\t Training Loss: 0.05884459543059043 \t\t Training Accurcy: 0.5000214224558359\n",
            "\t\t Validation Loss: 0.09174735820373801 \t\t Validation Accurcy: 0.5\n",
            "\t\t ------- Epoch 5 ---------\n",
            "\t\t Training Loss: 0.04560560678465026 \t\t Training Accurcy: 0.5002142245583587\n",
            "\t\t Validation Loss: 0.15065409811090022 \t\t Validation Accurcy: 0.5003424658350748\n",
            "\t\t ------- Epoch 6 ---------\n",
            "\t\t Training Loss: 0.03520272093663381 \t\t Training Accurcy: 0.5015852617063171\n",
            "\t\t Validation Loss: 0.15065409811090022 \t\t Validation Accurcy: 0.5003424658350748\n",
            "\t\t ------- Epoch 7 ---------\n",
            "\t\t Training Loss: 0.03020881614062183 \t\t Training Accurcy: 0.503556127668755\n",
            "\t\t Validation Loss: 0.19591252500094614 \t\t Validation Accurcy: 0.503595891268286\n",
            "\t\t ------- Epoch 8 ---------\n",
            "\t\t Training Loss: 0.031080196584394836 \t\t Training Accurcy: 0.5068766082467028\n",
            "\t\t Validation Loss: 0.19591252500094614 \t\t Validation Accurcy: 0.503595891268286\n",
            "\t\t ------- Epoch 9 ---------\n",
            "\t\t Training Loss: 0.027267870760267172 \t\t Training Accurcy: 0.5096401050495305\n",
            "\t\t Validation Loss: 0.15378242105628312 \t\t Validation Accurcy: 0.5066780837839597\n",
            "\t\t ------- Epoch 10 ---------\n",
            "\t\t Training Loss: 0.02262451028898025 \t\t Training Accurcy: 0.5146958046012591\n",
            "\t\t Validation Loss: 0.15378242105628312 \t\t Validation Accurcy: 0.5066780837839597\n",
            "\t\t ------- Epoch 11 ---------\n",
            "\t\t Training Loss: 0.023618992015078537 \t\t Training Accurcy: 0.5216366800367066\n",
            "\t\t Validation Loss: 0.17840973644641595 \t\t Validation Accurcy: 0.5275684991111494\n",
            "\t\t ------- Epoch 12 ---------\n",
            "\t\t Training Loss: 0.026191360773631458 \t\t Training Accurcy: 0.5266281116080264\n",
            "\t\t Validation Loss: 0.17840973644641595 \t\t Validation Accurcy: 0.5275684991111494\n",
            "\t\t ------- Epoch 13 ---------\n",
            "\t\t Training Loss: 0.021107771951467003 \t\t Training Accurcy: 0.5324978640473798\n",
            "\t\t Validation Loss: 0.10617010727770432 \t\t Validation Accurcy: 0.5255993203553435\n",
            "\t\t ------- Epoch 14 ---------\n",
            "\t\t Training Loss: 0.02153279502047052 \t\t Training Accurcy: 0.5378748996449825\n",
            "\t\t Validation Loss: 0.10617010727770432 \t\t Validation Accurcy: 0.5255993203553435\n",
            "\t\t ------- Epoch 15 ---------\n",
            "\t\t Training Loss: 0.019383785807988226 \t\t Training Accurcy: 0.5451156884150926\n",
            "\t\t Validation Loss: 0.14771316360357614 \t\t Validation Accurcy: 0.5373002348901474\n",
            "\t\t ------- Epoch 16 ---------\n",
            "\t\t Training Loss: 0.018576823551528697 \t\t Training Accurcy: 0.5508354830251562\n",
            "\t\t Validation Loss: 0.14771316360357614 \t\t Validation Accurcy: 0.5373002348901474\n",
            "\t\t ------- Epoch 17 ---------\n",
            "\t\t Training Loss: 0.01785854748341916 \t\t Training Accurcy: 0.5569194592056667\n",
            "\t\t Validation Loss: 0.23227770866955189 \t\t Validation Accurcy: 0.556599608329061\n",
            "\t\t ------- Epoch 18 ---------\n",
            "\t\t Training Loss: 0.018829190249815893 \t\t Training Accurcy: 0.564770787520192\n",
            "\t\t Validation Loss: 0.23227770866955189 \t\t Validation Accurcy: 0.556599608329061\n",
            "\t\t ------- Epoch 19 ---------\n",
            "\t\t Training Loss: 0.017829907232178567 \t\t Training Accurcy: 0.5709458081611938\n",
            "\t\t Validation Loss: 0.11434187592909639 \t\t Validation Accurcy: 0.566431229551361\n",
            "\t\t ------- Epoch 20 ---------\n",
            "\t\t Training Loss: 0.021750748772929825 \t\t Training Accurcy: 0.5806555340151145\n",
            "\t\t Validation Loss: 0.11434187592909639 \t\t Validation Accurcy: 0.566431229551361\n",
            "\t\t ------- Epoch 21 ---------\n",
            "\t\t Training Loss: 0.01582132406419133 \t\t Training Accurcy: 0.5801467507847784\n",
            "\t\t Validation Loss: 0.07129763521606271 \t\t Validation Accurcy: 0.5857876768871529\n",
            "\t\t ------- Epoch 22 ---------\n",
            "\t\t Training Loss: 0.016087505956630284 \t\t Training Accurcy: 0.5947033047369635\n",
            "\t\t Validation Loss: 0.07129763521606271 \t\t Validation Accurcy: 0.5857876768871529\n",
            "\t\t ------- Epoch 23 ---------\n",
            "\t\t Training Loss: 0.015743844813104833 \t\t Training Accurcy: 0.5967330812079945\n",
            "\t\t Validation Loss: 0.1388218664765314 \t\t Validation Accurcy: 0.5892694122987251\n",
            "\t\t ------- Epoch 24 ---------\n",
            "\t\t Training Loss: 0.01651949338979546 \t\t Training Accurcy: 0.6023832517448612\n",
            "\t\t Validation Loss: 0.1388218664765314 \t\t Validation Accurcy: 0.5892694122987251\n",
            "\t\t ------- Epoch 25 ---------\n",
            "\t\t Training Loss: 0.012887968891904258 \t\t Training Accurcy: 0.6115145703207864\n",
            "\t\t Validation Loss: 0.14567454225842377 \t\t Validation Accurcy: 0.5949629053269347\n",
            "\t\t ------- Epoch 26 ---------\n",
            "\t\t Training Loss: 0.015991865714461137 \t\t Training Accurcy: 0.6052324379113564\n",
            "\t\t Validation Loss: 0.14567454225842377 \t\t Validation Accurcy: 0.5949629053269347\n",
            "\t\t ------- Epoch 27 ---------\n",
            "\t\t Training Loss: 0.01522378398947951 \t\t Training Accurcy: 0.6047665000012928\n",
            "\t\t Validation Loss: 0.13513287826779433 \t\t Validation Accurcy: 0.6017908143670592\n",
            "\t\t ------- Epoch 28 ---------\n",
            "\t\t Training Loss: 0.018002228158804087 \t\t Training Accurcy: 0.6146368926778857\n",
            "\t\t Validation Loss: 0.13513287826779433 \t\t Validation Accurcy: 0.6017908143670592\n",
            "\t\t ------- Epoch 29 ---------\n",
            "\t\t Training Loss: 0.01497854353256667 \t\t Training Accurcy: 0.6239663685553076\n",
            "\t\t Validation Loss: 0.13657425897295564 \t\t Validation Accurcy: 0.6160388154118028\n",
            "\t\t ------- Epoch 30 ---------\n",
            "\t\t Training Loss: 0.015078195198415523 \t\t Training Accurcy: 0.6357487150258045\n",
            "\t\t Validation Loss: 0.13657425897295564 \t\t Validation Accurcy: 0.6160388154118028\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(43)\n",
        "torch.manual_seed(43)\n",
        "\n",
        "# different combinations\n",
        "batch_size = [10]\n",
        "k = [[2,4]]\n",
        "n = [60]\n",
        "\n",
        "#through each hyperparameter\n",
        "for batch in batch_size:\n",
        "  for k1,k2 in k:\n",
        "    for n_size in n:\n",
        "      print('===' + str(batch) + '===' + str(k1) + '===' + str(k2) + '===' + str(n_size))\n",
        "      trainepoch_loss,trainepoch_accurcy,valepoch_loss,valepoch_accurcy,network = main(\n",
        "          args=argparse.Namespace(batch_size=batch,overfit = False,k1 = k1,k2 = k2,n1=n_size,n2=n_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_accuracy = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "metric = MulticlassAccuracy(num_classes=5).to(device)\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100)\n",
        "\n",
        "test_dataset = TextDataset(glove, \"test\")\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,batch_size=5,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "\n",
        "for i, (X_batch, y_batch) in enumerate(test_dataloader):\n",
        "  target = torch.tensor([[y] for y in y_batch]).to(torch.float).to(device)\n",
        "  X_batch.to(device)\n",
        "  output = network(X_batch)\n",
        "  probs = torch.sigmoid(output)\n",
        "  test_accuracy.append(metric(probs, target).item())\n",
        "  \n",
        "print('the final test accuracy is: ' + str(np.mean(test_accuracy)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
